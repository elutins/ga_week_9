{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Datetime with Pandas\n",
    "\n",
    "\n",
    "## Framing\n",
    "\n",
    "This week we'll be working with time and space data. This lesson will focus on the basics of manipulating time data in python. You may have had some experience working with time data in the West Nile Virus thorugh trying to match weather data with trap inspection dates.  What are some other potential uses of time data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/nONbcdJqgP4NG/giphy.gif\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Outline\n",
    "\n",
    "- Introduce/Review Datetime Functions in Pandas\n",
    "- Generate plots with timedata\n",
    "- Introduce common datetime functions:\n",
    "\n",
    "    \n",
    "## Objectives:\n",
    "   \n",
    "By the end of this lesson, students will be able to:\n",
    " \n",
    "- Define and utilize the following datetime functions\n",
    "    - Resampling\n",
    "    - Autocorrelation\n",
    "    - Rolling Mean\n",
    "    - Explonential Mean\n",
    "    - Diff\n",
    "    - Expanding Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datetime with Pandas\n",
    "![pandas time!](https://s-media-cache-ak0.pinimg.com/originals/c2/65/44/c2654482bb9711cbd0cc4b5b7111f66e.jpg)\n",
    "\n",
    "\n",
    "Pandas started out in the financial industry (specifically at AQR Capital Management in 2008), which is why it's so great at everything related to timeseries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Datetime Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The date time library is something you should already have because of Anaconda.\n",
    "from datetime import datetime\n",
    "\n",
    "# Let's look at the date we once believed the world would end on.\n",
    "lesson_date = datetime(2012, 12, 21, 12, 21, 12, 844089)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Micro-Second\", lesson_date.microsecond\n",
    "print \"Second\", lesson_date.second\n",
    "print \"Minute\", lesson_date.minute\n",
    "print \"Hour\", lesson_date.hour\n",
    "print \"Day\", lesson_date.day\n",
    "print \"Month\",lesson_date.month\n",
    "print \"Year\", lesson_date.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timedelta\n",
    "Say we want to add or subtract time to/from a date. Perhaps we're using time as an index and we want to get everything that happened a week before a specific observation, for example.\n",
    "\n",
    "We can use a timedelta object to shift a Datetime object. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import timedelta from datetime library\n",
    "from datetime import timedelta\n",
    "\n",
    "# Time deltas represent time as an amount as opposed to a fixed position.\n",
    "offset = timedelta(days=1, seconds=20)\n",
    "\n",
    "# the time delta has attributes that allow us to extract values from it.\n",
    "print 'offset days', offset.days\n",
    "print 'offset seconds', offset.seconds\n",
    "print 'offset microseconds', offset.microseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "print \"It's now: \", now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Future: \", now + offset\n",
    "print \"Past: \", now - offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The largest value a time delta can hold is 'Days'.  I.e. you can't say you want you an offset to be 2 years, 44 days and 12 hours.  You would have to manually convert the time of those years to be represented in days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a dataset from the internets\n",
    "import pandas as pd\n",
    "ufo = pd.read_csv('http://bit.ly/uforeports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the Time column is just an object.\n",
    "ufo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overwrite the original Time column with one that has been converted to a datetime series.\n",
    "ufo['Time'] = pd.to_datetime(ufo.Time)\n",
    "\n",
    "#Letting pandas guess how to do this can take a little bit of time we can use a few arguments to help.\n",
    "'''ufo['Time'] = pd.to_datetime(ufo.Time, format='%Y%m%d', errors='coerce')'''\n",
    "# Format will let pandas know what format pandas should use to interpret the date as\n",
    "# errors will allow you to automatically deal with errors when converting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the time column looks a bit different now!\n",
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take a look at how the series has changed\n",
    "ufo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use dt to get weekday names \n",
    "ufo.Time.dt.weekday_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and what day of the year it was!\n",
    "ufo.Time.dt.dayofyear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ufo['weekend'] = [1 if x.weekday()>4 else 0 for x in ufo['Time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independent activity:\n",
    "Take 10 minutes to look at the different ways you can work with timezones and timezone formatting. Try creating a few new columns for things like daylight savings adjustment, timezone name, etc.\n",
    "\n",
    "https://docs.python.org/2/library/datetime.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ufo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create a timestamp of interest\n",
    "ts = pd.to_datetime('9/10/1993')\n",
    "#^that's the day x-files first came out, for all of you wondering\n",
    "ts\n",
    "# The main difference between a Datetime object and a timestamp is...\n",
    "# that timestamps can be used as comparisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the timestamp we just saved to create a new dataframe.\n",
    "ufo.loc[ufo.Time >= ts, :].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we could create a new column looking at how far away from our point of interest a particular UFO was sighted\n",
    "ufo['new'] = ufo.Time - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timedelta can also be used to get the min and max of a timeseries.\n",
    "ufo.Time.max() - ufo.Time.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use timedelta to mess around with the silly YouTube videos you're embedding in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"hAAlDoAtV7Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=int(timedelta(minutes=1, seconds=2).total_seconds())\n",
    "YouTubeVideo(\"hAAlDoAtV7Y\", start=start, autoplay=1, theme=\"light\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More independent work: \n",
    "\n",
    "Search for .dt. on http://pandas.pydata.org/pandas-docs/stable/api.html for more information about pandas Datetime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a timeseries using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's load in a different dataset\n",
    "crime = pd.read_csv('https://raw.githubusercontent.com/rufuspollock/crime-data-sf/gh-pages/data/sfpd_incidents_march_2012.tidied.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a look at our different types\n",
    "crime.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so do we want to mess around with the date or the time?\n",
    "crime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's turn date into a datetime object\n",
    "crime['Date'] = pd.to_datetime(crime.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I'm arbitrarily picking weekday to be how we look at our data\n",
    "crime['weekday'] = crime.Date.dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's groupby weekday on this \n",
    "crime_ts = crime.groupby('weekday').aggregate(len)['IncidntNum']\n",
    "#the groupby statement automatically makes weekday the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(crime_ts.index, crime_ts.values, lw=5)\n",
    "#LW = line width!\n",
    "#a small stringed instrument! a classical timeseries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: When working with datetime data, it is often best to convert that column to an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's convert the date to be the index\n",
    "crime.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crime['Month'] = crime.index.month\n",
    "crime['weekday'] = crime.index.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering by date becomes really easy when you're working with it as an index!\n",
    "\n",
    "crime['2012-03-04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#including looking at a range of observations\n",
    "crime['3/3/2012':'3/4/2012']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Introduction to autocorrelation and window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For this example will be working with sales data. This is a big dataset, so it might take a minute...\n",
    "url = 'https://raw.githubusercontent.com/sinanuozdemir/sfdat22/master/data/rossmann.csv'\n",
    "data = pd.read_csv(url, skipinitialspace=True, low_memory=False)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most interested in date - format properly and convert to index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns for year and month \n",
    "data['Year'] = data.index.year\n",
    "data['Month'] = data.index.month\n",
    "\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are over a million sales data points in this dataset, so for some simple EDA we will focus on just one store.\n",
    "store1_data = data[data.Store == 1]\n",
    "store1_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we begin to study the sales from this drugstore, we also want to know both the time dependent elements of sales as well as whether promotions or holidays effected these sales. To start, we can compare the average sales on those events.\n",
    "\n",
    "To compare sales on holidays, we can compare the sales using box-plots, which allows us to compare the distribution of sales on holidays against all other days. On state holidays the store is closed (which means there are 0 sales), and \n",
    "on school holidays the sales are relatively similar. These types of insights represent the contextual knowledge needed \n",
    "to truly explain time series phenomenon. Can you think of any other special considerations we should make when tracking sales?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check similarity between School Holiday and Sales\n",
    "sns.factorplot(\n",
    "    x='SchoolHoliday',\n",
    "    y='Sales',\n",
    "    data=store1_data,\n",
    "    kind='box'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We can see that there is a difference in sales on promotion days\n",
    "sns.factorplot(\n",
    "    col='Open',\n",
    "    x='Promo',\n",
    "    y='Sales',\n",
    "    data=store1_data,\n",
    "    kind='box'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it important to separate out days where the store is closed? Because there aren't any promotions on those days either, so including them will bias your sales data on days without promotions! \n",
    "\n",
    "Remember: Data Scientists needs to think about the business logic (context) as well as analyzing the raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perhaps plot sales across day of the week\n",
    "sns.factorplot(\n",
    "    col='Open',\n",
    "    x='DayOfWeek',\n",
    "    y='Sales',\n",
    "    data=store1_data,\n",
    "    kind='box',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider sales across multiple years. How did sales change from 2014 to 2015?\n",
    "\n",
    "# Filter to days store 1 was open\n",
    "store1_open_data = store1_data[store1_data.Open==1]\n",
    "store1_open_data[['Sales']].plot()          # sales over time\n",
    "store1_open_data[['Customers']].plot()      # customers over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class exercise: Use filtering to show the trend in 2015 alone\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is autocorrelation?\n",
    "\n",
    "Autocorrelation features measure the statistical correlation of a time series with a _lagged_ version of itself.\n",
    "\n",
    "**Check:** Why would we want to use autocorrelation? What are we trying to find?\n",
    "\n",
    "### Computing Autocorrelation\n",
    "To measure how much the sales are correlated with each other, we want to compute the autocorrelation of the 'Sales' column. In pandas, we'll do this with the **autocorr** function.\n",
    "\n",
    "\n",
    "**Autocorr takes one argument**\n",
    "\n",
    "_the lag:_ how many prior data points should be used to compute the correlation.\n",
    "\n",
    "If we set the lag to 1, we compute the correlation between every point and the point directly preceding it, If we set lag to 10, this computes the correlation between every point  and the point 10 days earlier\n",
    "\n",
    "**Check:** How is correlation mesaured? What is the range of values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sales'].resample('D').mean().autocorr(lag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#that's a pretty small mean correlation. what if we look at the autocorrelation for 30 days\n",
    "\n",
    "\n",
    "data['Sales'].resample('D').mean().autocorr(lag=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#What value of lag would be best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample\n",
    "\n",
    "If we want to investigate trends over time in sales, as always, we will start by computing simple aggregates. We want to know: what were the mean and median sales in each month and year?\n",
    "\n",
    "In Pandas, this is performed using the resample command, which is very similar to the groupby command. It allows us to group over different time intervals.\n",
    "\n",
    "We can use data.resample and provide as arguments: - The level on which to roll-up to, 'D' for day, 'W' for week, 'M' for month, 'A' for year - The aggregation to perform: 'mean', 'median', 'sum', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Sales']].resample('A').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.resample('A').mean()    # whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[['Sales']].resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to have the daily total over all stores\n",
    "# Alternatively, this could a daily average over all store with how='mean'\n",
    "\n",
    "daily_store_sales = data[['Sales']].resample('D').mean()\n",
    "daily_store_sales.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Rolling Mean\n",
    "\n",
    "What is a rolling mean, where might it be useful?\n",
    "\n",
    "\n",
    "We can use the pandas rolling_mean method to calculate the rolling mean.\n",
    "\n",
    "**Check**: Look at the documentation and tell us what \"window\" and \"center\" are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3-day rolling mean of daily store sales\n",
    "pd.rolling_mean(daily_store_sales, window=3, center=True)\n",
    "pd.rolling_mean(daily_store_sales, window=3, center=True)['2015']   # filter to 2015 only\n",
    "pd.rolling_mean(daily_store_sales, window=10, center=True).plot()   # plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also use the exponentially weighted moving average.\n",
    "\n",
    "**Check:** What is the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.ewma(data['Sales'], span=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Window Functions\n",
    "\n",
    "Pandas rolling_mean and rolling_median are only two examples of Pandas window function capabilities. Window functions operate on a set of N consecutive rows (i.e.: a window) and produce an output. In addition to rolling_mean and rolling_median, there are rolling_sum, rolling_min, rolling_max... and many more.\n",
    "\n",
    "Another common one is **diff**, which takes the difference over time. \n",
    "\n",
    "**pd.diff** takes one argument: _periods_, which measures how many rows prior to use for the difference.\n",
    "\n",
    "For example, if we want to compute the difference in sales, day by day, we could compute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily_store_sales.diff(periods=1) # day by day difference in sales\n",
    "daily_store_sales.diff(periods=7) # compare same day each week\n",
    "\n",
    "# Difference functions allow us to identify seasonal changes when we see repeated up or downswings.\n",
    "# An example from FiveThirtyEight:\n",
    "# http://i2.wp.com/espnfivethirtyeight.files.wordpress.com/2015/03/casselman-datalab-wsj2.png?quality=90&strip=all&w=575&ssl=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Expanding Functions\n",
    "In addition to the set of rolling functions, Pandas also provides a similar collection of expanding functions, which, instead of using a window of N values, uses all values up until that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.expanding_mean(daily_store_sales) # average date from first till last date specified\n",
    "pd.expanding_sum(daily_store_sales) # sum of average sales per store until that date\n",
    "\n",
    "#Check - what will these plots look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review:\n",
    "\n",
    "We've covered a lot this afternoon. Let's review all of the terms and ideas we discussed:\n",
    "\n",
    "- Using datetime format in Pandas - Setting the index\n",
    "- Resampling\n",
    "- Autocorrelation\n",
    "- Window Functions\n",
    "    - Rolling Mean\n",
    "    - Explonential Mean\n",
    "    - Diff\n",
    "- Expanding Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISES\n",
    "\n",
    "1. Plot the distribution of sales by month and compare the effect of promotions.\n",
    "hint: try using hue in sns\n",
    "2. Are sales more correlated with the prior date, a similar date last year, or a similar date last month?\n",
    "4. Identify the date with largest drop in sales from the same date in the previous week.\n",
    "5. Compute the total sales up until Dec. 2014.\n",
    "6. When were the largest differences between 15-day moving/rolling averages? HINT: Using rolling_mean and diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of sales by month and compare the effect of promotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Are sales more correlated with the prior date, a similar date last year, or a similar date last month?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify the date with largest drop in average store sales from the same date in the previous month:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the total sales up until Dec. 2014:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When were the largest differences between 15-day moving/rolling averages? HINT: Using rolling_mean and diff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming note on using time series for Capstones:\n",
    "\n",
    "Here's an example of using timeseries for a Capstone: https://github.com/samuel-stack/Portfolio/blob/master/Moving%20Violations%20VS.%20Speed%20Traps/Granger%20Causality%20test%20.ipynb\n",
    "\n",
    "Note, this Capstone makes use of Granger Causality: a statistical concept that says if a signal X \"Granger-causes\" (or \"G-causes\") a signal Y, then past values of X should contain information that helps predict Y above and beyond the information contained in past values of Y alone. \n",
    "\n",
    "To put it another way, a time series X1 is said to Granger-cause Y if the X1 values provide statistically significant information about future values of Y. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
